Efficient Memory Management for Large Language Model Serving with PagedAttention  
Authors: Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica

[1mAbstract[0m  
vLLM introduces PagedAttention, a novel attention mechanism inspired by virtual memory paging, enabling efficient key-value (KV) cache management for large language model (LLM) serving. This allows for near-zero memory waste and flexible KV cache sharing. Built atop PagedAttention, vLLM achieves up to 4Ã— throughput improvements over FasterTransformer and Orca, especially with large models and complex decoding.

[1m1. Introduction[0m  
LLM serving is costly and memory-bound. The main bottleneck is managing dynamic KV caches. Existing systems allocate contiguous memory, resulting in internal and external fragmentation. vLLM proposes PagedAttention to manage KV cache more like OS paging, improving batching and reducing cost.

[1m2. Background[0m  
Covers Transformers, KV cache role in autoregressive generation, and batching challenges. Batching must handle asynchronous arrivals and sequence length variance. Iteration-level scheduling helps reduce queueing and padding inefficiencies.

[1m4. Method[0m  
[1m4.1 PagedAttention[0m  
Enables KV cache to be divided into small, fixed-size blocks (KV blocks), stored non-contiguously.

[1m4.2 KV Cache Manager[0m  
Inspired by OS virtual memory: separates logical vs physical KV blocks using block tables, supports dynamic allocation.

[1m4.4 Decoding Scenarios[0m  
Supports complex tasks like parallel sampling and beam search with memory reuse and copy-on-write. Shared prefixes are handled efficiently.

[1m4.5 Scheduling and Preemption[0m  
Implements FCFS scheduling and preemption strategies using block eviction. Supports both swapping (CPU RAM) and recomputation as recovery mechanisms.

[1m4.6 Distributed Execution[0m  
vLLM supports model parallelism (e.g., Megatron-LM) by using a centralized KV cache manager and SPMD logic.

[1m8. Discussion[0m  
PagedAttention may not apply to compute-bound workloads, but is ideal for memory-bound LLM serving. Its kernel fusion techniques make the overhead manageable.

[1m10. Conclusion[0m  
vLLM introduces paging-inspired memory management to LLM serving. PagedAttention enables non-contiguous memory allocation and KV sharing, leading to substantial throughput improvements.